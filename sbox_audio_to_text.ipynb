{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 main steps \n",
    "# 1 - Google API (cost considerations)\n",
    "# 2 - HuggingFace\n",
    "# May requires translation, cutting podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3path = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\podcasts\\winter_break.mp3\"\n",
    "# 25 second long mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.askpython.com/python/environment-variables-in-python\n",
    "import os\n",
    "e = os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALLUSERSPROFILE',\n",
       " 'APPDATA',\n",
       " 'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL',\n",
       " 'CHROME_CRASHPAD_PIPE_NAME',\n",
       " 'CLICOLOR',\n",
       " 'CLICOLOR_FORCE',\n",
       " 'CLOUDSDK_METRICS_ENVIRONMENT',\n",
       " 'CLOUDSDK_METRICS_ENVIRONMENT_VERSION',\n",
       " 'CLOUDSDK_PYTHON',\n",
       " 'COMMONPROGRAMFILES',\n",
       " 'COMMONPROGRAMFILES(X86)',\n",
       " 'COMMONPROGRAMW6432',\n",
       " 'COMPUTERNAME',\n",
       " 'COMSPEC',\n",
       " 'CUDA_PATH',\n",
       " 'CUDA_PATH_V11_3',\n",
       " 'CUDA_PATH_V11_5',\n",
       " 'DRIVERDATA',\n",
       " 'ELECTRON_NO_ATTACH_CONSOLE',\n",
       " 'ELECTRON_RUN_AS_NODE',\n",
       " 'FORCE_COLOR',\n",
       " 'FPS_BROWSER_APP_PROFILE_STRING',\n",
       " 'FPS_BROWSER_USER_PROFILE_STRING',\n",
       " 'GIT_LFS_PATH',\n",
       " 'GIT_PAGER',\n",
       " 'HADOOP_HOME',\n",
       " 'HOMEDRIVE',\n",
       " 'HOMEPATH',\n",
       " 'JAVA_HOME',\n",
       " 'JPY_INTERRUPT_EVENT',\n",
       " 'LOCALAPPDATA',\n",
       " 'LOGONSERVER',\n",
       " 'MPLBACKEND',\n",
       " 'NUMBER_OF_PROCESSORS',\n",
       " 'NVCUDASAMPLES11_3_ROOT',\n",
       " 'NVCUDASAMPLES11_5_ROOT',\n",
       " 'NVCUDASAMPLES_ROOT',\n",
       " 'NVTOOLSEXT_PATH',\n",
       " 'ONEDRIVE',\n",
       " 'ORIGINAL_XDG_CURRENT_DESKTOP',\n",
       " 'OS',\n",
       " 'PAGER',\n",
       " 'PATH',\n",
       " 'PATHEXT',\n",
       " 'PROCESSOR_ARCHITECTURE',\n",
       " 'PROCESSOR_IDENTIFIER',\n",
       " 'PROCESSOR_LEVEL',\n",
       " 'PROCESSOR_REVISION',\n",
       " 'PROGRAMDATA',\n",
       " 'PROGRAMFILES',\n",
       " 'PROGRAMFILES(X86)',\n",
       " 'PROGRAMW6432',\n",
       " 'PROMPT',\n",
       " 'PSMODULEPATH',\n",
       " 'PUBLIC',\n",
       " 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING',\n",
       " 'PYDEVD_USE_FRAME_EVAL',\n",
       " 'PYTHONIOENCODING',\n",
       " 'PYTHONUNBUFFERED',\n",
       " 'SESSIONNAME',\n",
       " 'SPARK_HOME',\n",
       " 'SYSTEMDRIVE',\n",
       " 'SYSTEMROOT',\n",
       " 'TEMP',\n",
       " 'TERM',\n",
       " 'TMP',\n",
       " 'USERDOMAIN',\n",
       " 'USERDOMAIN_ROAMINGPROFILE',\n",
       " 'USERNAME',\n",
       " 'USERPROFILE',\n",
       " 'VIRTUAL_ENV',\n",
       " 'VIRTUAL_ENV_PROMPT',\n",
       " 'VS140COMNTOOLS',\n",
       " 'VSCODE_AMD_ENTRYPOINT',\n",
       " 'VSCODE_CLI',\n",
       " 'VSCODE_CODE_CACHE_PATH',\n",
       " 'VSCODE_CWD',\n",
       " 'VSCODE_HANDLES_UNCAUGHT_ERRORS',\n",
       " 'VSCODE_IPC_HOOK',\n",
       " 'VSCODE_L10N_BUNDLE_LOCATION',\n",
       " 'VSCODE_NLS_CONFIG',\n",
       " 'VSCODE_PID',\n",
       " 'WINDIR',\n",
       " 'ZES_ENABLE_SYSMAN',\n",
       " '_OLD_VIRTUAL_PATH',\n",
       " '_OLD_VIRTUAL_PROMPT']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(e.__dict__['_data'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\GCP_100Days\\speech_to_text\\LG_SpeechText_Credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "from google.cloud import speech_v1p1beta1 as speech\n",
    "\n",
    "\n",
    "def sample_recognize(storage_uri):\n",
    "    \"\"\"\n",
    "    Performs synchronous speech recognition on an audio file\n",
    "\n",
    "    Args:\n",
    "      storage_uri URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]\n",
    "    \"\"\"\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # storage_uri = 'gs://cloud-samples-data/speech/brooklyn_bridge.mp3'\n",
    "\n",
    "    # The language of the supplied audio\n",
    "    language_code = \"en-US\"\n",
    "\n",
    "    # Sample rate in Hertz of the audio data sent\n",
    "    sample_rate_hertz = 44100\n",
    "\n",
    "    # Encoding of audio data sent. This sample sets this explicitly.\n",
    "    # This field is optional for FLAC and WAV audio formats.\n",
    "    encoding = speech.RecognitionConfig.AudioEncoding.MP3\n",
    "    config = {\n",
    "        \"language_code\": language_code,\n",
    "        \"sample_rate_hertz\": sample_rate_hertz,\n",
    "        \"encoding\": encoding,\n",
    "    }\n",
    "    audio = {\"uri\": storage_uri}\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/speech-to-text/docs/samples/speech-quickstart-beta\n",
    "def sample_recognize(storage_uri):\n",
    "    \"\"\"\n",
    "    Performs synchronous speech recognition on an audio file\n",
    "\n",
    "    Args:\n",
    "      storage_uri URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]\n",
    "    \"\"\"\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # storage_uri = 'gs://cloud-samples-data/speech/brooklyn_bridge.mp3'\n",
    "\n",
    "    # The language of the supplied audio\n",
    "    language_code = \"en-US\"\n",
    "\n",
    "    # Sample rate in Hertz of the audio data sent\n",
    "    sample_rate_hertz = 44100\n",
    "\n",
    "    # Encoding of audio data sent. This sample sets this explicitly.\n",
    "    # This field is optional for FLAC and WAV audio formats.\n",
    "    encoding = speech.RecognitionConfig.AudioEncoding.MP3\n",
    "    config = {\n",
    "        \"language_code\": language_code,\n",
    "        \"sample_rate_hertz\": sample_rate_hertz,\n",
    "        \"encoding\": encoding,\n",
    "    }\n",
    "    audio = {\"uri\": storage_uri}\n",
    "\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_uri =  r\"gs://advance-anvil-169016-audio-files/winter_break.mp3\"\n",
    "response = sample_recognize(storage_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[alternatives {\n",
       "  transcript: \"hey everyone thank you so much for listening to Google Cloud Reader will be taking a break as we get ready for 2023 and all that it\\'s got in store for us will be back in January and until then we hope you get a chance to spend some time doing what you love most thanks for listening see you next time\"\n",
       "  confidence: 0.969436884\n",
       "}\n",
       "result_end_time {\n",
       "  seconds: 22\n",
       "  nanos: 340000000\n",
       "}\n",
       "language_code: \"en-us\"\n",
       "]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: hey everyone thank you so much for listening to Google Cloud Reader will be taking a break as we get ready for 2023 and all that it's got in store for us will be back in January and until then we hope you get a chance to spend some time doing what you love most thanks for listening see you next time\n"
     ]
    }
   ],
   "source": [
    "for result in response.results:\n",
    "    # First alternative is the most probable result\n",
    "    alternative = result.alternatives[0]\n",
    "    print(\"Transcript: {}\".format(alternative.transcript))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does open source pretrained models work as effectively,,, not tech or buzz words here, but a good starting place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "API_TOKEN = Path(r'.\\artifacts\\tokens\\hugging_face_api_token.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3path = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\podcasts\\winter_break.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h\"\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "data = query(mp3path)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"HAY EVERY ONE THANK YOU SO MUCH FOR LISTENING TO GUGGLECLOUD READER WE'LL BE TAKING A BRAG AS WE GET READY FOR TWENTY TWENTY THREE AND ALL THAT IT'S GOT IN STORE FOR US WE'LL BE BACK IN JANUARY AND UNTIL THEN WE HOPE YOU GET A CHANCE TO SPEND SOME TIME DOING WHAT YOU LOVE MOST THANKS FOR LISTENING SING NEXT TIME\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"say next time i every one think you so much for listening to grigal cloud reader we'll be taking a brig as we get ready for twenty twenty three and all that it's got in store for us will we back in january and until then we hope you get a chance to spend some doing what you love most thanks for listening say next time say next time say next time say next time say next time say next time\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/s2t-small-librispeech-asr\"\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "data = query(mp3path)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Hey jeder, vielen Dank und bis dahin wird es Ihnen im Januar sehr Zeit sein, alles zu tun, was Sie beim n√§chsten Mal in Ordnung sind.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/s2t-wav2vec2-large-en-de\"\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "data = query(mp3path)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" Hey everyone, thank you so much for listening to Google Cloud Reader. We'll be taking a break as we get ready for 2023 and all that it's got in store for us. We'll be back in January and until then, we hope you get a chance to spend some time doing what you love most. Thanks for listening. See you next time.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openai/whisper-large\"\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n",
    "data = query(mp3path)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whisper - Inference is currently only implemented for short-form i.e. audio is pre-segmented into <=30s segments. Long-form (including timestamps) will be implemented in a future release.\n",
    "\n",
    "https://huggingface.co/openai/whisper-large\n",
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\") # 6GV monster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None  #  the context tokens are 'unforced', meaning the model automatically predicts the output language (English) and task (transcribe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3path = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\podcasts\\winter_break.mp3\"\n",
    "from whisper.audio import load_audio\n",
    "data = load_audio(mp3path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "input_features = processor(audio=data, return_tensors=\"pt\").input_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(inputs=input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hey everyone, thank you so much for listening to Google Cloud Reader. We'll be taking a break as we get ready for 2023 and all that it's got in store for us. We'll be back in January and until then, we hope you get a chance to spend some time doing what you love most. Thanks for listening. See you next time.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to path: C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\ffmpeg-master-latest-win64-gpl-shared\\bin\n",
    "import sys\n",
    "sys.path.append(r\"#add to path: C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\ffmpeg-master-latest-win64-gpl-shared\\bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey everyone, thank you so much for listening to Google Cloud Reader. We'll be taking a break as we get ready for 2023 and all that it's got in store for us. We'll be back in January and until then, we hope you get a chance to spend some time doing what you love most. Thanks for listening, see you next time.\n"
     ]
    }
   ],
   "source": [
    "mp3path = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\podcasts\\winter_break.mp3\"\n",
    "result = model.transcribe(mp3path)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3800430\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "mp3path = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\podcasts\\SGTTU\\clean-code-in-python.mp3\"\n",
    "#mp3path = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\podcasts\\winter_break.mp3\"\n",
    "sound = AudioSegment.from_mp3(mp3path)\n",
    "print(len(sound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3_800_430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633405.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3_800_430/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\podcasts\\SGTTU\\clean-code-in-python_10min.mp3\"\n",
    "sound_slice = sound[:600_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='C:\\\\Users\\\\lgarzia\\\\Documents\\\\GitHub\\\\topic_extractions\\\\podcasts\\\\SGTTU\\\\clean-code-in-python_10min.mp3'>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_slice.export(fout, format='mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\venv\\lib\\site-packages\\whisper\\transcribe.py:79: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clean code is one of those aspects of your programming career that's easy to put on the back burner, sometimes more by management than yourself, but it's important in the short term for writing more debuggable and readable code, and it's important in the long run for avoiding having your program take on the dreaded Legacy Code Monitor. We're fortunate to have Bob Biltervos back on the show. He's been thinking and writing about clean code and Python a lot lately, and we'll dive into a bunch of tips that you can use right away to make your code cleaner, more fun to work with, easy to read, and more maintainable. This is Talk Python to me, episode 404, recorded February 12th, 2023. Welcome to Talk Python to me, a weekly podcast on Python. This is your host, Michael Kennedy. Follow me on Macedon, where I'm at M. Kennedy and follow the podcast using at Talk Python, both on bossstaton.org. Be careful with in-personity and accounts on other instances, there are many. Keep up with the show and listen to over seven years of past episodes at Talk Python.fm. We've started streaming most of our episodes live on YouTube. Subscribe to our YouTube channel over at TalkPython.fm.slash YouTube to get notified of about upcoming shows and be part of that episode. This episode of Talk Python may be brought to you by TyPy. They're here to take on the challenge of rapidly transforming a bare algorithm and Python into a full-fledged, decision support system. Check them out at Talk Python.fm.slash TyPy, T-A-I-P-Y, and it's also brought to you by my friends over at Brilliant. Stay on top of technology and raise your value to employers or just learn something fun and STEM at Brilliant.org. Visit Talk Python.fm.slash Brilliant to get 20% off an annual premium subscription. Bob, welcome back to Talk Python to me. Thanks for having me back. I'm excited to be here. Yeah, it's really great to have you back. You've been on a couple of times. We started, I believe our first discussion on the show Way Back When was around 100 days of code. We went on quite the 100 days of code journey, writing a couple of really, really long courses that were well received, but it took a long, you know, like nine months for us to write, which was amazing. And we're back together. It's great to catch up. Yeah, indeed. Yeah, that goes way back to 100 days of Python and two courses that came from it. And I think we also did a Django episode. Yeah. That's right. We did do a Django episode as well. Good stuff. So this time, we're back to talk about code quality, writing clean code, what are some of the tools, and also what are some of the techniques, maybe even mindset people are using to help them write clean code, better code, both for yourself to make yourself happy and as well to be a better teammate if you're working in the team, right? Yeah. This topic really excites me. It's in my everyday work. And I think there's a lot to game from it. So yeah, excited to share today. I am too. It's something I really, really care a lot about. It's one of those topics that I think is long lived, which is rare in software development. It's not the latest JavaScript framework or something that's going to last for a year or two. These ideas are the kind of ideas that just, no matter what you're doing, even if you don't do Python, if you do something else, it's very likely that these are solid foundations. Some of the ideas will be specifically for Python, but many of them not. Yeah, timeless stuff. Yeah, those are the good ones that are worth putting your time into learning. Now, before we jump into that topic, which is going to be good, maybe just tell people what you're up to. These days, so I left Oracle while back. I think we spoke about that in 2020. So I've been working on PyBytes full times for almost three years. Congratulations. I know you and I spoke about this beforehand, and it was a really big step for you. It was. Something you were absolutely looking forward to, but just walking away from a good paying job, that's scary. So congratulations. Yeah, thanks. This is also around the time that as PyBytes we pivoted to doing coaching, so helping people, one-to-one, we created our PyBytes developer mindset program, a 12-week coaching program, and that really took off. We have work now with 100-plus people, and but I'm super excited about every day when I wake up because helping people overcoming the tutorial of practice is embracing imposter syndrome, helping them shipping two projects and it's just the best thing ever. Yeah. You see people grow and become more confident and get better, and yeah, it's really, it's excellent work, isn't it? Yeah, it's really fulfilling. Yeah. So congratulations. It's awesome to hear that you're doing that now, and you can put all of your energy, not just your side hustle energy into these types of things, which I could tell you, feels real good. You know how it is, right? I do, I do. I've been in it for a while. Very fortunate. Okay. So let's start this off with looking at your article, but before we do, Tune Army Captain has a really great way to sort of kick off this topic, I believe. Clean code has less or fewer WTFs per line about that. Nice. So yeah, that's, I think that's going to be a theme here. Let's go ahead and jump into it. I reached out to you and said, hey, let's get together and talk about this because I knew that you were passionate about these things, but then you also put together a blog post here called Tips for Clean Code in Python. I thought, okay, that's interesting. Let me start flipping through here. And there's just a bunch of nice ideas that resonate with me. So what I thought we could do is maybe you could set the stage, like what the heck is Clean Code? Tune Army did do a pretty good job of kind of giving us the colloquial term, but you'll maybe something a little more safe for a conformal we could go with. Set the stage for what is Clean Code? Why does it matter? And then we could dive into some of the stuff you put into your article. And then also you and I both threw some ideas in after it, like to kind of expand it as well. So let's let's start with what is this Clean Code thing? Yeah. So set off the stage. So Clean Code is really when you write code that's easier to maintain, easier to test, easier to update, because as we all know, a project usually starts simple, but over time it grows, new requirements comes in. You constantly have to change things. And yeah, the cleanness of your code will definitely determine how easy you can move a project forward. How you can make changes. And of course, it also contribution to your team to make it a more pleasant experience and decrease the amount of WTFs. Yeah, you don't want to fewer the better, although maybe you'll even want to and just for a good war story at a conference five years from now, but you don't want too many of those. And I think it's appreciating the fact that if you do a good job, what you're building will live a long time, right? It will possibly be still the main focus in five years from now. And you don't always know what that's going to be, right? It could be, oh, here's a little side project. It's just going to pull in some data and like, you know, you show it to somebody at work and like, you know what? That's great. Shipping. Like, whoa, whoa, whoa, whoa, whoa, ship it. No, no, I just threw it together. It's it's good. Let's go. Like we're in a hurry, you know, and then that just starts to build and layer on like a sediment that just makes it harder and less fun to work with. And so as you just think about this thing is small now, but as it grows, what are the both the practices and maybe technological programming things to bring in put in place so that as it grows, you don't start to just really come to a screeching halt to make changes. It's test start to fail randomly as you just touch it in various places, things like that, right? Yeah. It was also kind of my introduction to Python in 2012. So almost 11 years ago when I made this automation framework at work and I was a big fan of Pearl back then, I guess that goes back to my Unix shell scripting initial programming exploration. And it was not maintainable at all, right? There were no classes in Pearl and it was just a mess. Also, of course, because I did no Pearl, maybe very well. But then I discovered Python and I could refactor it, make it more modular and that project was then just a joy to work with. And maybe that says a lot about Python as well that it does inherently things well because if you type in port this into REPL and you get the center of Python, a lot of these statements sound so simple that they're actually so profound and they actually tell you a lot about writing clean code in a sense. So anyway, but I digress. They do. They also talk a lot about the community that cares enough that, you know, you have these sorts of commands in the language itself that remind you to think about writing clean code. And going way back to the late 80s, you know, when he came up with it, really broke with tradition to use white space for the code structure. And I think it's a little bit less so now, but back then that was very much to encourage people to write code in a way that was very readable. Whereas, you know, I say less so than now because you've got a lot of IDEs and stuff. We can push a button and it'll auto format it and you've got things like black. And, you know, if you had curly races, like it could still be quickly put into a pretty shape now. But in the early days, that was a really important concept of it. These days were spoiled, right? Having black out of formatting and all that. And when we started, that was not the case. How did you react to the white space thing when you first started it? I just really didn't like it. I thought this is just weird. Like, okay, everything else about this language seems pretty nice. But this is weird stuff. Then I started using editors that understood it. So like, okay, well, I just hit, you know, colon and enter in it. It kind of does it for me automatically. And sure, there's three spaces. I four spaces, but if I have backspace, then it goes back forward. It kind of treats it the structure. The tools know the structure. And it makes it really nice. And at the time, this is the long as doing C sharp, which is an okay language. It's pretty good. But it has all the symbols and formulas and all the.\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(fout)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed attempt below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53633177/how-to-read-a-mp3-audio-file-into-a-numpy-array-save-a-numpy-array-to-mp3\n",
    "mp3path = r\"C:\\Users\\lgarzia\\Documents\\GitHub\\topic_extractions\\podcasts\\winter_break.mp3\"\n",
    "import pydub \n",
    "import numpy as np\n",
    "def read(f, normalized=False):\n",
    "    \"\"\"MP3 to numpy array\"\"\"\n",
    "    a = pydub.AudioSegment.from_mp3(f)\n",
    "    y = np.array(a.get_array_of_samples())\n",
    "    if a.channels == 2:\n",
    "        y = y.reshape((-1, 2))\n",
    "    if normalized:\n",
    "        return a.frame_rate, np.float32(y) / 2**15\n",
    "    else:\n",
    "        return a.frame_rate, y\n",
    "data = read(mp3path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50c2f3fdaff21cf524066e6f7d306be0b9f081a5cbf5acb3eba5a76347fee337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
